{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58c6606",
   "metadata": {},
   "source": [
    "# Model Uncertainty Analysis in Nonlinear Optimization\n",
    "\n",
    "Understanding parameter uncertainties is crucial in nonlinear inverse problems because:\n",
    "1. **Data contain noise** ‚Üí Parameter estimates contain uncertainty\n",
    "2. **Nonlinear models** ‚Üí Uncertainty propagation is complex\n",
    "3. **Decision making** requires confidence intervals and reliability estimates\n",
    "\n",
    "## üéØ Overview of Uncertainty Methods\n",
    "\n",
    "| Method | Type | Computational Cost | Assumptions | Best For |\n",
    "|--------|------|-------------------|-------------|----------|\n",
    "| **Linear Approximation** | Local | Low | Near-linear behavior | Quick estimates |\n",
    "| **Bootstrap** | Resampling | High | None | Robust estimates |\n",
    "| **Jackknife** | Resampling | Medium | Smooth statistics | Bias correction |\n",
    "| **Monte Carlo** | Simulation | Very High | Known data distribution | Full uncertainty |\n",
    "\n",
    "---\n",
    "\n",
    "## 1. üìê Local (Approximate) Linearization Methods\n",
    "\n",
    "### 1.1 Jacobian-Based Covariance Estimation\n",
    "\n",
    "For a nonlinear least-squares problem: $E(\\mathbf{m}) = ||\\mathbf{d} - \\mathbf{g}(\\mathbf{m})||^2$\n",
    "\n",
    "**Theoretical Foundation:**\n",
    "- **Assumption**: Near the minimum, the objective function behaves quadratically\n",
    "- **Linear approximation**: $\\mathbf{g}(\\mathbf{m}) \\approx \\mathbf{g}(\\mathbf{m}_0) + \\mathbf{J}(\\mathbf{m} - \\mathbf{m}_0)$\n",
    "- **Result**: Parameter covariance matrix becomes analytically tractable\n",
    "\n",
    "**Mathematical Framework:**\n",
    "\n",
    "The **Jacobian matrix** at the solution $\\hat{\\mathbf{m}}$:\n",
    "$$\\mathbf{J}_{ij} = \\frac{\\partial g_i}{\\partial m_j}\\bigg|_{\\mathbf{m}=\\hat{\\mathbf{m}}}$$\n",
    "\n",
    "**Parameter covariance matrix**:\n",
    "$$\\mathbf{C}_m = \\sigma^2 (\\mathbf{J}^T\\mathbf{J})^{-1}$$\n",
    "\n",
    "where $\\sigma^2$ is the data variance estimate:\n",
    "$$\\sigma^2 = \\frac{E(\\hat{\\mathbf{m}})}{N - M}$$\n",
    "- $N$ = number of data points\n",
    "- $M$ = number of model parameters\n",
    "\n",
    "**Parameter standard errors**:\n",
    "$$\\sigma_{m_i} = \\sqrt{[\\mathbf{C}_m]_{ii}}$$\n",
    "\n",
    "**95% Confidence intervals**:\n",
    "$$\\hat{m}_i \\pm 1.96 \\sigma_{m_i}$$\n",
    "\n",
    "### 1.2 Finite-Difference Hessian Method\n",
    "\n",
    "When analytical derivatives are difficult, use **numerical approximation**:\n",
    "\n",
    "**Hessian matrix** (second derivatives):\n",
    "$$H_{ij} = \\frac{\\partial^2 E}{\\partial m_i \\partial m_j}$$\n",
    "\n",
    "**Finite-difference approximation**:\n",
    "$$H_{ij} \\approx \\frac{E(m_i + \\epsilon, m_j + \\epsilon) - E(m_i + \\epsilon, m_j - \\epsilon) - E(m_i - \\epsilon, m_j + \\epsilon) + E(m_i - \\epsilon, m_j - \\epsilon)}{4\\epsilon^2}$$\n",
    "\n",
    "**Parameter covariance**:\n",
    "$$\\mathbf{C}_m = \\frac{1}{2}\\mathbf{H}^{-1}$$\n",
    "\n",
    "**Advantages**: No analytical derivatives required\n",
    "**Disadvantages**: $(M^2)$ function evaluations needed\n",
    "\n",
    "---\n",
    "\n",
    "## 2. üîÑ Bootstrap Resampling Method\n",
    "\n",
    "### 2.1 How Bootstrap Works\n",
    "\n",
    "#### üîπ The Analogy\n",
    "\n",
    "**\"The sample is to the population as the bootstrap sample is to the sample.\"**\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\\text{Population} \\longrightarrow \\text{Sample} \\longrightarrow \\text{Bootstrap Sample}$$\n",
    "\n",
    "This expresses a nested analogy of how information flows:\n",
    "\n",
    "| Level | What it Represents | What We Know | What We Want |\n",
    "|-------|-------------------|--------------|--------------|\n",
    "| **Population** | The real world (true distribution $F$) | Unknown | Theoretical sampling distribution of our estimator |\n",
    "| **Sample** | Our finite dataset (empirical distribution $\\hat{F}$) | Known | Approximation of the population |\n",
    "| **Bootstrap Sample** | A resample from the sample (with replacement) | We can generate many | Approximation of how the estimator would vary across repeated samples from $F$ |\n",
    "\n",
    "#### üîπ In Words\n",
    "\n",
    "1. **We only have one dataset** from an unknown population.\n",
    "\n",
    "2. If we could **repeatedly sample from the true population**, we'd see how our estimator (mean, regression coefficient, slip rate, etc.) fluctuates ‚Äî that's the **true sampling distribution**.\n",
    "\n",
    "3. **But we can't resample the population** ‚Äî it's gone! \n",
    "\n",
    "4. So we **pretend the sample we have is a mini-version of the population**.\n",
    "\n",
    "5. Then:\n",
    "   - Each **bootstrap sample** (drawn with replacement from our data) **mimics what would happen** if we took a new sample from the population.\n",
    "\n",
    "So, conceptually:\n",
    "\n",
    "$$\\text{Population} \\xrightarrow{\\text{sample once}} \\text{Sample} \\xrightarrow{\\text{resample many times}} \\text{Bootstrap samples}$$\n",
    "\n",
    "and the **distribution of estimates from bootstrap samples** approximates the **distribution of estimates from true samples**.\n",
    "\n",
    "#### üîπ Example\n",
    "\n",
    "Imagine measuring **vertical GPS velocities at 20 stations** near a fault.\n",
    "\n",
    "- The **true region** (all possible GPS sites) = **population**\n",
    "- Your **20 observed stations** = **sample**  \n",
    "- Each **bootstrap resample** (20 points drawn with replacement from those 20) = **bootstrap sample**\n",
    "\n",
    "You fit your fault-slip model to each bootstrap sample. The **spread of estimated slip rates** across those resamples tells you:\n",
    "\n",
    "*\"If I had gone back and measured a different random set of 20 GPS stations from the same population, how much might my estimated slip rate have varied?\"*\n",
    "\n",
    "**That's the bootstrap principle in action.**\n",
    "\n",
    "#### üîπ Bootstrap Algorithm\n",
    "\n",
    "**Step-by-step procedure**:\n",
    "1. **Original dataset**: $\\{\\mathbf{d}, \\mathbf{t}\\}$ with $N$ observations\n",
    "2. **Create bootstrap sample**: Randomly sample $N$ points **with replacement**\n",
    "3. **Solve inverse problem**: Find $\\hat{\\mathbf{m}}_b$ for bootstrap sample\n",
    "4. **Repeat**: Generate $B$ bootstrap samples (typically $B = 1000-10000$)\n",
    "5. **Analyze distribution**: Compute statistics from $\\{\\hat{\\mathbf{m}}_1, \\hat{\\mathbf{m}}_2, ..., \\hat{\\mathbf{m}}_B\\}$\n",
    "\n",
    "### 2.2 Mathematical Foundation\n",
    "\n",
    "**Central Limit Theorem Application**:\n",
    "If $\\hat{\\mathbf{m}}$ is an estimator of $\\mathbf{m}_{true}$, then for large sample sizes:\n",
    "\n",
    "$$\\hat{\\mathbf{m}} \\sim \\mathcal{N}(\\mathbf{m}_{true}, \\mathbf{C}_m)$$\n",
    "\n",
    "**Bootstrap estimators**:\n",
    "\n",
    "**Mean**: $\\bar{\\mathbf{m}}_{boot} = \\frac{1}{B}\\sum_{b=1}^{B} \\hat{\\mathbf{m}}_b$\n",
    "\n",
    "**Covariance**: $\\mathbf{C}_{boot} = \\frac{1}{B-1}\\sum_{b=1}^{B} (\\hat{\\mathbf{m}}_b - \\bar{\\mathbf{m}}_{boot})(\\hat{\\mathbf{m}}_b - \\bar{\\mathbf{m}}_{boot})^T$\n",
    "\n",
    "**Confidence intervals**: Use empirical quantiles from bootstrap distribution\n",
    "\n",
    "### 2.3 Bootstrap Advantages\n",
    "\n",
    "‚úÖ **Model-free**: No assumptions about parameter distributions  \n",
    "‚úÖ **Nonlinear-friendly**: Captures asymmetric uncertainties  \n",
    "‚úÖ **Realistic**: Accounts for actual data distribution  \n",
    "‚úÖ **Flexible**: Works with any optimization algorithm  \n",
    "‚úÖ **Bias correction**: Can detect and correct estimator bias  \n",
    "\n",
    "### 2.4 Bootstrap Limitations\n",
    "\n",
    "‚ùå **Computational cost**: Requires $B$ complete optimizations  \n",
    "‚ùå **Sample dependence**: Quality depends on original sample representativeness  \n",
    "‚ùå **Convergence issues**: Each bootstrap optimization must converge  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. üî™ Jackknife Method\n",
    "\n",
    "### 3.1 Jackknife Procedure\n",
    "\n",
    "**Leave-one-out resampling**:\n",
    "1. **Original dataset**: $N$ observations\n",
    "2. **Create jackknife sample**: Remove observation $i$, keep remaining $N-1$\n",
    "3. **Solve**: Find $\\hat{\\mathbf{m}}_{-i}$ using reduced dataset\n",
    "4. **Repeat**: For all $i = 1, 2, ..., N$ (exactly $N$ samples)\n",
    "5. **Compute statistics**: Analyze $\\{\\hat{\\mathbf{m}}_{-1}, \\hat{\\mathbf{m}}_{-2}, ..., \\hat{\\mathbf{m}}_{-N}\\}$\n",
    "\n",
    "### 3.2 Jackknife Estimators\n",
    "\n",
    "**Bias-corrected estimate**:\n",
    "$$\\hat{\\mathbf{m}}_{jack} = N\\hat{\\mathbf{m}} - \\frac{N-1}{N}\\sum_{i=1}^{N} \\hat{\\mathbf{m}}_{-i}$$\n",
    "\n",
    "**Variance estimate**:\n",
    "$$\\text{Var}_{jack}(\\hat{\\mathbf{m}}) = \\frac{N-1}{N}\\sum_{i=1}^{N} (\\hat{\\mathbf{m}}_{-i} - \\bar{\\mathbf{m}}_{jack})^2$$\n",
    "\n",
    "where $\\bar{\\mathbf{m}}_{jack} = \\frac{1}{N}\\sum_{i=1}^{N} \\hat{\\mathbf{m}}_{-i}$\n",
    "\n",
    "### 3.3 Jackknife vs Bootstrap\n",
    "\n",
    "| Aspect | Jackknife | Bootstrap |\n",
    "|--------|-----------|-----------|\n",
    "| **Samples** | Exactly $N$ | Typically $B >> N$ |\n",
    "| **Sampling** | Deterministic | Random |\n",
    "| **Bias correction** | Built-in | Optional |\n",
    "| **Computational cost** | Lower | Higher |\n",
    "| **Uncertainty estimates** | More conservative | More detailed |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. üé≤ Theoretical Comparison\n",
    "\n",
    "### 4.1 When Each Method Works Best\n",
    "\n",
    "**Linear Approximation**:\n",
    "- ‚úÖ Objective function is nearly quadratic near minimum\n",
    "- ‚úÖ Large datasets (asymptotic regime)\n",
    "- ‚úÖ Quick uncertainty estimates needed\n",
    "\n",
    "**Bootstrap**:\n",
    "- ‚úÖ Nonlinear problems with complex parameter distributions\n",
    "- ‚úÖ Sufficient computational resources\n",
    "- ‚úÖ Robust uncertainty quantification needed\n",
    "\n",
    "**Jackknife**:\n",
    "- ‚úÖ Small to moderate datasets\n",
    "- ‚úÖ Bias correction important\n",
    "- ‚úÖ Computational budget limited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca466046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Uncertainty Analysis for Nonlinear Optimization\n",
    "# Forward model: d = sin(w0*m1*x) + m1*m2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"UNCERTAINTY ANALYSIS FOR NONLINEAR OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Forward model: d = sin(w0*m1*x) + m1*m2\")\n",
    "print(f\"True parameters: m1 = {mtrue[0]:.6f}, m2 = {mtrue[1]:.6f}\")\n",
    "print(f\"Data: {len(dobs)} observations with noise\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. GAUSS-NEWTON OPTIMIZATION WITH JACOBIAN-BASED UNCERTAINTY\n",
    "# ============================================================================\n",
    "\n",
    "def forward_model(m, x_data):\n",
    "    \"\"\"Forward model: d = sin(w0*m1*x) + m1*m2\"\"\"\n",
    "    m1, m2 = m[0], m[1]\n",
    "    return np.sin(w0 * m1 * x_data) + m1 * m2\n",
    "\n",
    "def jacobian_matrix(m, x_data):\n",
    "    \"\"\"Analytical Jacobian matrix for the forward model\"\"\"\n",
    "    m1, m2 = m[0], m[1]\n",
    "    n_data = len(x_data)\n",
    "    J = np.zeros((n_data, 2))\n",
    "    \n",
    "    # Partial derivatives\n",
    "    J[:, 0] = w0 * x_data * np.cos(w0 * m1 * x_data) + m2  # ‚àÇg/‚àÇm1\n",
    "    J[:, 1] = m1 * np.ones(n_data)                           # ‚àÇg/‚àÇm2\n",
    "    \n",
    "    return J\n",
    "\n",
    "def gauss_newton_with_uncertainty(x_data, d_obs, m_init, max_iter=20, tol=1e-8):\n",
    "    \"\"\"Gauss-Newton with uncertainty estimation\"\"\"\n",
    "    m_current = m_init.copy()\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Forward prediction\n",
    "        d_pred = forward_model(m_current, x_data)\n",
    "        residuals = d_obs - d_pred\n",
    "        \n",
    "        # Jacobian matrix\n",
    "        J = jacobian_matrix(m_current, x_data)\n",
    "        \n",
    "        # Gauss-Newton update\n",
    "        try:\n",
    "            JTJ = J.T @ J\n",
    "            JTr = J.T @ residuals\n",
    "            dm = np.linalg.solve(JTJ, JTr)\n",
    "            m_current = m_current + dm\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(dm) < tol:\n",
    "                print(f\"Gauss-Newton converged in {iteration+1} iterations\")\n",
    "                break\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Singular matrix encountered\")\n",
    "            break\n",
    "    \n",
    "    # Final residuals and Jacobian for uncertainty\n",
    "    d_pred_final = forward_model(m_current, x_data)\n",
    "    residuals_final = d_obs - d_pred_final\n",
    "    J_final = jacobian_matrix(m_current, x_data)\n",
    "    \n",
    "    # Uncertainty estimation\n",
    "    n_data = len(d_obs)\n",
    "    n_params = len(m_current)\n",
    "    \n",
    "    # Data variance estimate\n",
    "    sigma_squared = np.sum(residuals_final**2) / (n_data - n_params)\n",
    "    \n",
    "    # Parameter covariance matrix\n",
    "    try:\n",
    "        C_m = sigma_squared * np.linalg.inv(J_final.T @ J_final)\n",
    "        param_errors = np.sqrt(np.diag(C_m))\n",
    "        correlation_matrix = C_m / np.outer(param_errors, param_errors)\n",
    "    except np.linalg.LinAlgError:\n",
    "        C_m = None\n",
    "        param_errors = None\n",
    "        correlation_matrix = None\n",
    "        print(\"Warning: Could not compute covariance matrix\")\n",
    "    \n",
    "    return {\n",
    "        'solution': m_current,\n",
    "        'residuals': residuals_final,\n",
    "        'jacobian': J_final,\n",
    "        'covariance': C_m,\n",
    "        'param_errors': param_errors,\n",
    "        'correlation': correlation_matrix,\n",
    "        'sigma_squared': sigma_squared,\n",
    "        'rms_error': np.sqrt(np.mean(residuals_final**2))\n",
    "    }\n",
    "\n",
    "# Run Gauss-Newton optimization\n",
    "x_data = np.linspace(0, 1.0, len(dobs))\n",
    "m_initial = np.array([1.0, 1.5])  # Initial guess\n",
    "\n",
    "print(f\"\\n--- GAUSS-NEWTON OPTIMIZATION ---\")\n",
    "gn_result = gauss_newton_with_uncertainty(x_data, dobs, m_initial)\n",
    "\n",
    "print(f\"Final solution: m1 = {gn_result['solution'][0]:.6f}, m2 = {gn_result['solution'][1]:.6f}\")\n",
    "print(f\"True values:    m1 = {mtrue[0]:.6f}, m2 = {mtrue[1]:.6f}\")\n",
    "print(f\"Parameter errors: œÉ_m1 = {gn_result['param_errors'][0]:.6f}, œÉ_m2 = {gn_result['param_errors'][1]:.6f}\")\n",
    "print(f\"RMS residual: {gn_result['rms_error']:.6f}\")\n",
    "print(f\"Data variance estimate: œÉ¬≤ = {gn_result['sigma_squared']:.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. BOOTSTRAP UNCERTAINTY ESTIMATION\n",
    "# ============================================================================\n",
    "\n",
    "def bootstrap_uncertainty(x_data, d_obs, n_bootstrap=1000, random_seed=42):\n",
    "    \"\"\"Bootstrap uncertainty estimation\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    n_data = len(d_obs)\n",
    "    bootstrap_solutions = []\n",
    "    successful_fits = 0\n",
    "    \n",
    "    print(f\"\\n--- BOOTSTRAP UNCERTAINTY ANALYSIS ---\")\n",
    "    print(f\"Generating {n_bootstrap} bootstrap samples...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for b in range(n_bootstrap):\n",
    "        # Create bootstrap sample (sample with replacement)\n",
    "        bootstrap_indices = np.random.choice(n_data, size=n_data, replace=True)\n",
    "        x_boot = x_data[bootstrap_indices]\n",
    "        d_boot = d_obs[bootstrap_indices]\n",
    "        \n",
    "        # Optimize on bootstrap sample\n",
    "        try:\n",
    "            # Use a slightly perturbed initial guess for robustness\n",
    "            m_init_boot = m_initial + 0.01 * np.random.randn(2)\n",
    "            boot_result = gauss_newton_with_uncertainty(x_boot, d_boot, m_init_boot)\n",
    "            \n",
    "            # Check if solution is reasonable\n",
    "            if (boot_result['solution'][0] > 0.5 and boot_result['solution'][0] < 2.5 and\n",
    "                boot_result['solution'][1] > 0.5 and boot_result['solution'][1] < 3.0):\n",
    "                bootstrap_solutions.append(boot_result['solution'])\n",
    "                successful_fits += 1\n",
    "                \n",
    "        except:\n",
    "            continue  # Skip failed optimizations\n",
    "        \n",
    "        if (b + 1) % 100 == 0:\n",
    "            print(f\"  Completed {b+1}/{n_bootstrap} bootstrap samples...\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    bootstrap_solutions = np.array(bootstrap_solutions)\n",
    "    \n",
    "    print(f\"Bootstrap completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successful fits: {successful_fits}/{n_bootstrap} ({100*successful_fits/n_bootstrap:.1f}%)\")\n",
    "    \n",
    "    # Bootstrap statistics\n",
    "    if len(bootstrap_solutions) > 0:\n",
    "        boot_mean = np.mean(bootstrap_solutions, axis=0)\n",
    "        boot_std = np.std(bootstrap_solutions, axis=0, ddof=1)\n",
    "        boot_cov = np.cov(bootstrap_solutions.T)\n",
    "        \n",
    "        # Confidence intervals (percentile method)\n",
    "        ci_lower = np.percentile(bootstrap_solutions, 2.5, axis=0)\n",
    "        ci_upper = np.percentile(bootstrap_solutions, 97.5, axis=0)\n",
    "        \n",
    "        return {\n",
    "            'solutions': bootstrap_solutions,\n",
    "            'mean': boot_mean,\n",
    "            'std': boot_std,\n",
    "            'covariance': boot_cov,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'n_successful': successful_fits\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Run bootstrap analysis\n",
    "bootstrap_result = bootstrap_uncertainty(x_data, dobs, n_bootstrap=500)\n",
    "\n",
    "if bootstrap_result is not None:\n",
    "    print(f\"Bootstrap mean:    m1 = {bootstrap_result['mean'][0]:.6f}, m2 = {bootstrap_result['mean'][1]:.6f}\")\n",
    "    print(f\"Bootstrap std:     œÉ_m1 = {bootstrap_result['std'][0]:.6f}, œÉ_m2 = {bootstrap_result['std'][1]:.6f}\")\n",
    "    print(f\"95% CI for m1: [{bootstrap_result['ci_lower'][0]:.6f}, {bootstrap_result['ci_upper'][0]:.6f}]\")\n",
    "    print(f\"95% CI for m2: [{bootstrap_result['ci_lower'][1]:.6f}, {bootstrap_result['ci_upper'][1]:.6f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. COMPREHENSIVE VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot 1: Objective function surface with uncertainties\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "contour = ax1.contour(M2_grid, M1_grid, E, levels=30, alpha=0.6)\n",
    "ax1.contourf(M2_grid, M1_grid, E, levels=50, cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Plot true solution\n",
    "ax1.plot(mtrue[1], mtrue[0], '*', color='gold', markersize=20, \n",
    "         markeredgecolor='black', markeredgewidth=2, label='True solution')\n",
    "\n",
    "# Plot Gauss-Newton solution\n",
    "ax1.plot(gn_result['solution'][1], gn_result['solution'][0], 'o', \n",
    "         color='red', markersize=12, markeredgecolor='black', \n",
    "         markeredgewidth=2, label='Gauss-Newton')\n",
    "\n",
    "# Plot 2-sigma confidence ellipse (if covariance available)\n",
    "if gn_result['covariance'] is not None:\n",
    "    # Extract 2x2 covariance matrix and compute ellipse\n",
    "    cov_2x2 = gn_result['covariance']\n",
    "    \n",
    "    # Eigenvalues and eigenvectors for ellipse orientation\n",
    "    eigenvals, eigenvecs = np.linalg.eig(cov_2x2)\n",
    "    \n",
    "    # 2-sigma ellipse parameters\n",
    "    chi2_2sigma = 5.991  # Chi-squared value for 2-sigma, 2 DOF\n",
    "    \n",
    "    # Ellipse semi-axes\n",
    "    a = np.sqrt(chi2_2sigma * eigenvals[0])  # Semi-major axis\n",
    "    b = np.sqrt(chi2_2sigma * eigenvals[1])  # Semi-minor axis\n",
    "    \n",
    "    # Ellipse angle\n",
    "    angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))\n",
    "    \n",
    "    # Create ellipse\n",
    "    from matplotlib.patches import Ellipse\n",
    "    ellipse = Ellipse((gn_result['solution'][1], gn_result['solution'][0]), \n",
    "                     2*b, 2*a, angle=angle, \n",
    "                     fill=False, color='red', linewidth=3, \n",
    "                     linestyle='--', alpha=0.8, label='2œÉ confidence ellipse')\n",
    "    ax1.add_patch(ellipse)\n",
    "\n",
    "# Plot bootstrap samples\n",
    "if bootstrap_result is not None:\n",
    "    ax1.scatter(bootstrap_result['solutions'][:, 1], bootstrap_result['solutions'][:, 0],\n",
    "               c='white', s=8, alpha=0.6, edgecolors='black', linewidth=0.5,\n",
    "               label=f'Bootstrap samples (n={bootstrap_result[\"n_successful\"]})')\n",
    "\n",
    "ax1.set_xlabel('m‚ÇÇ', fontsize=12)\n",
    "ax1.set_ylabel('m‚ÇÅ', fontsize=12)\n",
    "ax1.set_title('Objective Surface with Uncertainties', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Bootstrap parameter histograms\n",
    "if bootstrap_result is not None:\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    ax2.hist(bootstrap_result['solutions'][:, 0], bins=30, alpha=0.7, \n",
    "             color='blue', edgecolor='black', density=True, label='Bootstrap m‚ÇÅ')\n",
    "    ax2.axvline(mtrue[0], color='gold', linestyle='-', linewidth=3, label='True m‚ÇÅ')\n",
    "    ax2.axvline(gn_result['solution'][0], color='red', linestyle='--', linewidth=2, label='GN m‚ÇÅ')\n",
    "    ax2.axvline(bootstrap_result['mean'][0], color='green', linestyle=':', linewidth=2, label='Bootstrap mean')\n",
    "    ax2.set_xlabel('m‚ÇÅ')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_title('Bootstrap Distribution: m‚ÇÅ')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    ax3.hist(bootstrap_result['solutions'][:, 1], bins=30, alpha=0.7, \n",
    "             color='green', edgecolor='black', density=True, label='Bootstrap m‚ÇÇ')\n",
    "    ax3.axvline(mtrue[1], color='gold', linestyle='-', linewidth=3, label='True m‚ÇÇ')\n",
    "    ax3.axvline(gn_result['solution'][1], color='red', linestyle='--', linewidth=2, label='GN m‚ÇÇ')\n",
    "    ax3.axvline(bootstrap_result['mean'][1], color='green', linestyle=':', linewidth=2, label='Bootstrap mean')\n",
    "    ax3.set_xlabel('m‚ÇÇ')\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.set_title('Bootstrap Distribution: m‚ÇÇ')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Data fit comparison\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "x_fine = np.linspace(0, 1.0, 100)\n",
    "\n",
    "# True model\n",
    "d_true_fine = np.sin(w0 * mtrue[0] * x_fine) + mtrue[0] * mtrue[1]\n",
    "ax4.plot(x_fine, d_true_fine, 'k-', linewidth=3, label='True model')\n",
    "\n",
    "# Observed data\n",
    "ax4.plot(x_data, dobs, 'ro', markersize=6, alpha=0.7, label='Observed data')\n",
    "\n",
    "# Gauss-Newton fit\n",
    "d_gn_fine = np.sin(w0 * gn_result['solution'][0] * x_fine) + gn_result['solution'][0] * gn_result['solution'][1]\n",
    "ax4.plot(x_fine, d_gn_fine, 'b--', linewidth=2, label='Gauss-Newton fit')\n",
    "\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('d')\n",
    "ax4.set_title('Data Fit')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Uncertainty comparison\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "methods = ['Jacobian', 'Bootstrap']\n",
    "m1_errors = [gn_result['param_errors'][0], bootstrap_result['std'][0] if bootstrap_result else 0]\n",
    "m2_errors = [gn_result['param_errors'][1], bootstrap_result['std'][1] if bootstrap_result else 0]\n",
    "\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax5.bar(x_pos - width/2, m1_errors, width, label='œÉ_m‚ÇÅ', alpha=0.7, color='blue')\n",
    "bars2 = ax5.bar(x_pos + width/2, m2_errors, width, label='œÉ_m‚ÇÇ', alpha=0.7, color='green')\n",
    "\n",
    "ax5.set_xlabel('Method')\n",
    "ax5.set_ylabel('Parameter Error')\n",
    "ax5.set_title('Uncertainty Estimates Comparison')\n",
    "ax5.set_xticks(x_pos)\n",
    "ax5.set_xticklabels(methods)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Annotate bars\n",
    "for bar, error in zip(bars1, m1_errors):\n",
    "    height = bar.get_height()\n",
    "    ax5.annotate(f'{error:.4f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "for bar, error in zip(bars2, m2_errors):\n",
    "    height = bar.get_height()\n",
    "    ax5.annotate(f'{error:.4f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "# Plot 6: Correlation analysis\n",
    "if bootstrap_result is not None and gn_result['covariance'] is not None:\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    \n",
    "    # Bootstrap correlation\n",
    "    boot_corr = bootstrap_result['covariance'][0, 1] / (bootstrap_result['std'][0] * bootstrap_result['std'][1])\n",
    "    \n",
    "    # Jacobian correlation\n",
    "    jacobian_corr = gn_result['correlation'][0, 1]\n",
    "    \n",
    "    correlations = [jacobian_corr, boot_corr]\n",
    "    ax6.bar(methods, correlations, alpha=0.7, color=['red', 'purple'])\n",
    "    ax6.set_xlabel('Method')\n",
    "    ax6.set_ylabel('Correlation (m‚ÇÅ, m‚ÇÇ)')\n",
    "    ax6.set_title('Parameter Correlation')\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, corr in enumerate(correlations):\n",
    "        ax6.annotate(f'{corr:.3f}', xy=(i, corr), xytext=(0, 3),\n",
    "                    textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"UNCERTAINTY ANALYSIS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\nüìä PARAMETER ESTIMATES:\")\n",
    "print(f\"{'Method':<15} {'m‚ÇÅ':<12} {'m‚ÇÇ':<12} {'œÉ_m‚ÇÅ':<12} {'œÉ_m‚ÇÇ':<12}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'True':<15} {mtrue[0]:<12.6f} {mtrue[1]:<12.6f} {'‚Äî':<12} {'‚Äî':<12}\")\n",
    "print(f\"{'Gauss-Newton':<15} {gn_result['solution'][0]:<12.6f} {gn_result['solution'][1]:<12.6f} \"\n",
    "      f\"{gn_result['param_errors'][0]:<12.6f} {gn_result['param_errors'][1]:<12.6f}\")\n",
    "\n",
    "if bootstrap_result is not None:\n",
    "    print(f\"{'Bootstrap':<15} {bootstrap_result['mean'][0]:<12.6f} {bootstrap_result['mean'][1]:<12.6f} \"\n",
    "          f\"{bootstrap_result['std'][0]:<12.6f} {bootstrap_result['std'][1]:<12.6f}\")\n",
    "\n",
    "print(f\"\\nüéØ PARAMETER ACCURACY:\")\n",
    "gn_error = np.linalg.norm(gn_result['solution'] - mtrue)\n",
    "print(f\"Gauss-Newton parameter error: {gn_error:.6f}\")\n",
    "\n",
    "if bootstrap_result is not None:\n",
    "    boot_error = np.linalg.norm(bootstrap_result['mean'] - mtrue)\n",
    "    print(f\"Bootstrap parameter error: {boot_error:.6f}\")\n",
    "\n",
    "print(f\"\\nüîó PARAMETER CORRELATIONS:\")\n",
    "if gn_result['correlation'] is not None:\n",
    "    print(f\"Jacobian-based correlation: {gn_result['correlation'][0,1]:.4f}\")\n",
    "\n",
    "if bootstrap_result is not None:\n",
    "    boot_corr = bootstrap_result['covariance'][0, 1] / (bootstrap_result['std'][0] * bootstrap_result['std'][1])\n",
    "    print(f\"Bootstrap correlation: {boot_corr:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ KEY FINDINGS:\")\n",
    "print(f\"‚Ä¢ Linear approximation {'works well' if abs(gn_result['correlation'][0,1]) < 0.5 else 'may be inadequate'}\")\n",
    "print(f\"‚Ä¢ Bootstrap provides {'similar' if abs(gn_result['param_errors'][0] - bootstrap_result['std'][0]) < 0.01 else 'different'} uncertainty estimates\")\n",
    "print(f\"‚Ä¢ Parameter correlation is {'weak' if abs(gn_result['correlation'][0,1]) < 0.3 else 'moderate' if abs(gn_result['correlation'][0,1]) < 0.7 else 'strong'}\")\n",
    "print(f\"‚Ä¢ Data fit quality: RMS residual = {gn_result['rms_error']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
