{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58c6606",
   "metadata": {},
   "source": [
    "# Model Uncertainty Analysis in Nonlinear Optimization\n",
    "\n",
    "Understanding parameter uncertainties is crucial in nonlinear inverse problems because:\n",
    "1. **Data contain noise** â†’ Parameter estimates contain uncertainty\n",
    "2. **Nonlinear models** â†’ Uncertainty propagation is complex\n",
    "3. **Decision making** requires confidence intervals and reliability estimates\n",
    "\n",
    "## ðŸŽ¯ Overview of Uncertainty Methods\n",
    "\n",
    "| Method | Type | Computational Cost | Assumptions | Best For |\n",
    "|--------|------|-------------------|-------------|----------|\n",
    "| **Linear Approximation** | Local | Low | Near-linear behavior | Quick estimates |\n",
    "| **Bootstrap** | Resampling | High | None | Robust estimates |\n",
    "| **Jackknife** | Resampling | Medium | Smooth statistics | Bias correction |\n",
    "| **Monte Carlo** | Simulation | Very High | Known data distribution | Full uncertainty |\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ðŸ“ Local (Approximate) Linearization Methods\n",
    "\n",
    "### 1.1 Jacobian-Based Covariance Estimation\n",
    "\n",
    "For a nonlinear least-squares problem: $E(\\mathbf{m}) = ||\\mathbf{d} - \\mathbf{g}(\\mathbf{m})||^2$\n",
    "\n",
    "**Theoretical Foundation:**\n",
    "- **Assumption**: Near the minimum, the objective function behaves quadratically\n",
    "- **Linear approximation**: $\\mathbf{g}(\\mathbf{m}) \\approx \\mathbf{g}(\\mathbf{m}_0) + \\mathbf{J}(\\mathbf{m} - \\mathbf{m}_0)$\n",
    "- **Result**: Parameter covariance matrix becomes analytically tractable\n",
    "\n",
    "**Mathematical Framework:**\n",
    "\n",
    "The **Jacobian matrix** at the solution $\\hat{\\mathbf{m}}$:\n",
    "$$\\mathbf{J}_{ij} = \\frac{\\partial g_i}{\\partial m_j}\\bigg|_{\\mathbf{m}=\\hat{\\mathbf{m}}}$$\n",
    "\n",
    "**Parameter covariance matrix**:\n",
    "$$\\mathbf{C}_m = \\sigma^2 (\\mathbf{J}^T\\mathbf{J})^{-1}$$\n",
    "\n",
    "where $\\sigma^2$ is the data variance estimate:\n",
    "$$\\sigma^2 = \\frac{E(\\hat{\\mathbf{m}})}{N - M}$$\n",
    "- $N$ = number of data points\n",
    "- $M$ = number of model parameters\n",
    "\n",
    "**Parameter standard errors**:\n",
    "$$\\sigma_{m_i} = \\sqrt{[\\mathbf{C}_m]_{ii}}$$\n",
    "\n",
    "**95% Confidence intervals**:\n",
    "$$\\hat{m}_i \\pm 1.96 \\sigma_{m_i}$$\n",
    "\n",
    "### 1.2 Finite-Difference Hessian Method\n",
    "\n",
    "When analytical derivatives are difficult, use **numerical approximation**:\n",
    "\n",
    "**Hessian matrix** (second derivatives):\n",
    "$$H_{ij} = \\frac{\\partial^2 E}{\\partial m_i \\partial m_j}$$\n",
    "\n",
    "**Finite-difference approximation**:\n",
    "$$H_{ij} \\approx \\frac{E(m_i + \\epsilon, m_j + \\epsilon) - E(m_i + \\epsilon, m_j - \\epsilon) - E(m_i - \\epsilon, m_j + \\epsilon) + E(m_i - \\epsilon, m_j - \\epsilon)}{4\\epsilon^2}$$\n",
    "\n",
    "**Parameter covariance**:\n",
    "$$\\mathbf{C}_m = \\frac{1}{2}\\mathbf{H}^{-1}$$\n",
    "\n",
    "**Advantages**: No analytical derivatives required\n",
    "**Disadvantages**: $(M^2)$ function evaluations needed\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ðŸ”„ Bootstrap Resampling Method\n",
    "\n",
    "### 2.1 How Bootstrap Works\n",
    "\n",
    "#### ðŸ”¹ The Analogy\n",
    "\n",
    "**\"The sample is to the population as the bootstrap sample is to the sample.\"**\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\\text{Population} \\longrightarrow \\text{Sample} \\longrightarrow \\text{Bootstrap Sample}$$\n",
    "\n",
    "This expresses a nested analogy of how information flows:\n",
    "\n",
    "| Level | What it Represents | What We Know | What We Want |\n",
    "|-------|-------------------|--------------|--------------|\n",
    "| **Population** | The real world (true distribution $F$) | Unknown | Theoretical sampling distribution of our estimator |\n",
    "| **Sample** | Our finite dataset (empirical distribution $\\hat{F}$) | Known | Approximation of the population |\n",
    "| **Bootstrap Sample** | A resample from the sample (with replacement) | We can generate many | Approximation of how the estimator would vary across repeated samples from $F$ |\n",
    "\n",
    "#### ðŸ”¹ In Words\n",
    "\n",
    "1. **We only have one dataset** from an unknown population.\n",
    "\n",
    "2. If we could **repeatedly sample from the true population**, we'd see how our estimator (mean, regression coefficient, slip rate, etc.) fluctuates â€” that's the **true sampling distribution**.\n",
    "\n",
    "3. **But we can't resample the population** â€” it's gone! \n",
    "\n",
    "4. So we **pretend the sample we have is a mini-version of the population**.\n",
    "\n",
    "5. Then:\n",
    "   - Each **bootstrap sample** (drawn with replacement from our data) **mimics what would happen** if we took a new sample from the population.\n",
    "\n",
    "So, conceptually:\n",
    "\n",
    "$$\\text{Population} \\xrightarrow{\\text{sample once}} \\text{Sample} \\xrightarrow{\\text{resample many times}} \\text{Bootstrap samples}$$\n",
    "\n",
    "and the **distribution of estimates from bootstrap samples** approximates the **distribution of estimates from true samples**.\n",
    "\n",
    "#### ðŸ”¹ Example\n",
    "\n",
    "Imagine measuring **vertical GPS velocities at 20 stations** near a fault.\n",
    "\n",
    "- The **true region** (all possible GPS sites) = **population**\n",
    "- Your **20 observed stations** = **sample**  \n",
    "- Each **bootstrap resample** (20 points drawn with replacement from those 20) = **bootstrap sample**\n",
    "\n",
    "You fit your fault-slip model to each bootstrap sample. The **spread of estimated slip rates** across those resamples tells you:\n",
    "\n",
    "*\"If I had gone back and measured a different random set of 20 GPS stations from the same population, how much might my estimated slip rate have varied?\"*\n",
    "\n",
    "**That's the bootstrap principle in action.**\n",
    "\n",
    "#### ðŸ”¹ Bootstrap Algorithm\n",
    "\n",
    "**Step-by-step procedure**:\n",
    "1. **Original dataset**: $\\{\\mathbf{d}, \\mathbf{t}\\}$ with $N$ observations\n",
    "2. **Create bootstrap sample**: Randomly sample $N$ points **with replacement**\n",
    "3. **Solve inverse problem**: Find $\\hat{\\mathbf{m}}_b$ for bootstrap sample\n",
    "4. **Repeat**: Generate $B$ bootstrap samples (typically $B = 1000-10000$)\n",
    "5. **Analyze distribution**: Compute statistics from $\\{\\hat{\\mathbf{m}}_1, \\hat{\\mathbf{m}}_2, ..., \\hat{\\mathbf{m}}_B\\}$\n",
    "\n",
    "### 2.2 Mathematical Foundation\n",
    "\n",
    "**Central Limit Theorem Application**:\n",
    "If $\\hat{\\mathbf{m}}$ is an estimator of $\\mathbf{m}_{true}$, then for large sample sizes:\n",
    "\n",
    "$$\\hat{\\mathbf{m}} \\sim \\mathcal{N}(\\mathbf{m}_{true}, \\mathbf{C}_m)$$\n",
    "\n",
    "**Bootstrap estimators**:\n",
    "\n",
    "**Mean**: $\\bar{\\mathbf{m}}_{boot} = \\frac{1}{B}\\sum_{b=1}^{B} \\hat{\\mathbf{m}}_b$\n",
    "\n",
    "**Covariance**: $\\mathbf{C}_{boot} = \\frac{1}{B-1}\\sum_{b=1}^{B} (\\hat{\\mathbf{m}}_b - \\bar{\\mathbf{m}}_{boot})(\\hat{\\mathbf{m}}_b - \\bar{\\mathbf{m}}_{boot})^T$\n",
    "\n",
    "**Confidence intervals**: Use empirical quantiles from bootstrap distribution\n",
    "\n",
    "### 2.3 Bootstrap Advantages\n",
    "\n",
    "âœ… **Model-free**: No assumptions about parameter distributions  \n",
    "âœ… **Nonlinear-friendly**: Captures asymmetric uncertainties  \n",
    "âœ… **Realistic**: Accounts for actual data distribution  \n",
    "âœ… **Flexible**: Works with any optimization algorithm  \n",
    "âœ… **Bias correction**: Can detect and correct estimator bias  \n",
    "\n",
    "### 2.4 Bootstrap Limitations\n",
    "\n",
    "âŒ **Computational cost**: Requires $B$ complete optimizations  \n",
    "âŒ **Sample dependence**: Quality depends on original sample representativeness  \n",
    "âŒ **Convergence issues**: Each bootstrap optimization must converge  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. ðŸ”ª Jackknife Method\n",
    "\n",
    "### 3.1 Jackknife Procedure\n",
    "\n",
    "**Leave-one-out resampling**:\n",
    "1. **Original dataset**: $N$ observations\n",
    "2. **Create jackknife sample**: Remove observation $i$, keep remaining $N-1$\n",
    "3. **Solve**: Find $\\hat{\\mathbf{m}}_{-i}$ using reduced dataset\n",
    "4. **Repeat**: For all $i = 1, 2, ..., N$ (exactly $N$ samples)\n",
    "5. **Compute statistics**: Analyze $\\{\\hat{\\mathbf{m}}_{-1}, \\hat{\\mathbf{m}}_{-2}, ..., \\hat{\\mathbf{m}}_{-N}\\}$\n",
    "\n",
    "### 3.2 Jackknife Estimators\n",
    "\n",
    "**Bias-corrected estimate**:\n",
    "$$\\hat{\\mathbf{m}}_{jack} = N\\hat{\\mathbf{m}} - \\frac{N-1}{N}\\sum_{i=1}^{N} \\hat{\\mathbf{m}}_{-i}$$\n",
    "\n",
    "**Variance estimate**:\n",
    "$$\\text{Var}_{jack}(\\hat{\\mathbf{m}}) = \\frac{N-1}{N}\\sum_{i=1}^{N} (\\hat{\\mathbf{m}}_{-i} - \\bar{\\mathbf{m}}_{jack})^2$$\n",
    "\n",
    "where $\\bar{\\mathbf{m}}_{jack} = \\frac{1}{N}\\sum_{i=1}^{N} \\hat{\\mathbf{m}}_{-i}$\n",
    "\n",
    "### 3.3 Jackknife vs Bootstrap\n",
    "\n",
    "| Aspect | Jackknife | Bootstrap |\n",
    "|--------|-----------|-----------|\n",
    "| **Samples** | Exactly $N$ | Typically $B >> N$ |\n",
    "| **Sampling** | Deterministic | Random |\n",
    "| **Bias correction** | Built-in | Optional |\n",
    "| **Computational cost** | Lower | Higher |\n",
    "| **Uncertainty estimates** | More conservative | More detailed |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ðŸŽ² Theoretical Comparison\n",
    "\n",
    "### 4.1 When Each Method Works Best\n",
    "\n",
    "**Linear Approximation**:\n",
    "- âœ… Objective function is nearly quadratic near minimum\n",
    "- âœ… Large datasets (asymptotic regime)\n",
    "- âœ… Quick uncertainty estimates needed\n",
    "\n",
    "**Bootstrap**:\n",
    "- âœ… Nonlinear problems with complex parameter distributions\n",
    "- âœ… Sufficient computational resources\n",
    "- âœ… Robust uncertainty quantification needed\n",
    "\n",
    "**Jackknife**:\n",
    "- âœ… Small to moderate datasets\n",
    "- âœ… Bias correction important\n",
    "- âœ… Computational budget limited\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
